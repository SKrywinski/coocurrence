{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import things needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to single files in a subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# code snippet to split the wikimedia corpus linewise into files, naming the files by the first two words in the 'article'\n",
    "with open('/home/hautakivi/Downloads/wiki-de.txt', 'r') as f:\n",
    "    for x in f:\n",
    "        x = x.replace('/','')\n",
    "        n = x.split()[:2]\n",
    "        n = '_'.join(n)\n",
    "        with open('/home/hautakivi/Downloads/txts/{}.txt'.format(n), 'w') as o:\n",
    "            o.write(x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in stopwordlist\n",
    "\n",
    "(Stopwordlist taken from {github...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r') as s:\n",
    "    e = s.readlines()\n",
    "stopwords = [x.strip() for x in e]\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define function that creates a dictionnary of lists, that is comparable to a kwic-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwic(c, string, k={}):\n",
    "    #k = {}\n",
    "    for n, e in enumerate(string):\n",
    "        if n - c <= 0:\n",
    "            #k[e]=string[l[:n+(c+1)]]\n",
    "            #k[e] = string[:n+(c+1)]\n",
    "            #print(n, e, string[:n+(c+1)])\n",
    "            if e not in k.keys():\n",
    "                k[e] = []\n",
    "                k[e] = string[:n+(c+1)]\n",
    "            else:\n",
    "                #k[e] = string[:n+(c+1)]\n",
    "                #k[e].append(string[:n+(c+1)])\n",
    "                k[e] += [string[:n+(c+1)]]\n",
    "        elif n + c > len(string)+1:\n",
    "            #k[e] = string[n-c:]\n",
    "            #print(n, e, string[n-c:])\n",
    "            if e not in k.keys():\n",
    "                k[e] = []\n",
    "                k[e] = string[n-c:]\n",
    "            else:\n",
    "                #k[e] = string[n-c:]\n",
    "                #k[e].append(string[n-c:])\n",
    "                k[e] += string[n-c:]\n",
    "        else:\n",
    "            #k[e] = string[n-c:n+(c+1)]\n",
    "            #print(n, e, string[n-c:n+(c+1)])\n",
    "            if e not in k.keys():\n",
    "                k[e] = []\n",
    "                k[e] = string[n-c:n+(c+1)]\n",
    "            else:\n",
    "                #k[e] = string[n-c:n+(c+1)]\n",
    "                #k[e].append(string[n-c:n+(c+1)])\n",
    "                k[e] += string[n-c:n+(c+1)]\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to serialize words to a file, avoid keeping them in memory\n",
    "\n",
    "First: create a complete wordlist of all files\n",
    "\n",
    "Second: build up words per documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all the texts and create list of words (later: not iterating twice over the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "with open('wiki-de.txt', 'r') as f:\n",
    "    # while loop: for developing purposes only read a number of lines, else replace the integer with for.\n",
    "    #for line in f:\n",
    "    i = 0\n",
    "    line = f.readline()\n",
    "    words = list((line.lower().split()))\n",
    "    wordList = [x for x in words if x not in stopwords]\n",
    "    ergebnis = kwic(c=2, string=wordList)\n",
    "    #print(ergebnis)\n",
    "    \n",
    "    wordCounter = Counter(wordList)\n",
    "    df = pd.DataFrame.from_dict(wordCounter, orient='index', columns=[i])\n",
    "    i+=1\n",
    "\n",
    "    while i < 10:\n",
    "        line = f.readline()\n",
    "        words = list((line.lower().split()))\n",
    "        wordList = [x for x in words if x not in stopwords]\n",
    "        ergebnis = kwic(c=2, string=wordList, k=ergebnis)\n",
    "        #print('{}.:\\t'.format(i), ergebnis['streng'])\n",
    "        \n",
    "        wordCounter = Counter(wordList)\n",
    "        df = pd.concat([df, pd.DataFrame.from_dict(wordCounter, orient='index', columns=[i])], axis=1)\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfE = pd.DataFrame.from_dict(ergebnis, orient='index')\n",
    "dfE.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = dfE.stack().reset_index()\n",
    "#without index-resetting\n",
    "#dfTest = dfE.stack()\n",
    "dfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMatrix = dfTest[:5000].sort_values('level_0').pivot_table(index='level_0', \n",
    "                              columns=0, \n",
    "                              aggfunc='count')\n",
    "dfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of \"titles\" (created from the first two words of the article) to have an overview of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "c = 0\n",
    "with open('wiki-de.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        c += 1\n",
    "'''\n",
    "liste = []\n",
    "with open('wiki-de.txt', 'r') as f:\n",
    "    for x in f:\n",
    "        x = x.replace('/','')\n",
    "        n = x.split()[:2]\n",
    "        n = '_'.join(n)\n",
    "        print(n)\n",
    "        liste.append(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n",
    "#c\n",
    "liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sum(axis=1).sort_values(ascending=False)[:10000]>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  --- Sandbox ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,2]\n",
    "[x for x in b if x not in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([[1,2],[3,4]],['wort1','wort2'],['text1','text2'])\n",
    "df2 = pd.DataFrame([[5,6],[3,4],[8,9]],['wort3','wort1','wort4'],['text1','text2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([1,2,99],['wort2','wort4','wort5'])\n",
    "dfS = pd.DataFrame([1,2,99],['wort2','wort4','wort5'],['text3'])\n",
    "dfS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'Sei dies jetzt mal ein Wikipedia Artikel in einer Zeile'\n",
    "p2 = 'Sei dies mal ein weiterer Eintrag in die Wikipedia'\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'aber':['kein', 'wenn', 'und', 'aber']}\n",
    "a2 = {'wenn':['kein', 'wenn', 'und', 'aber']}\n",
    "a['aber'] += a2['wenn']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = kwic(c=2, string=p.split(), k={})\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic(c=2, string=p2.split(), k=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
